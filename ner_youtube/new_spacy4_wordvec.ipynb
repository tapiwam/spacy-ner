{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb382f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e09b6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Corpus\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        data=f.read()\n",
    "    return data\n",
    "\n",
    "def save_data(file, data):\n",
    "    \n",
    "    \n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# full_book = './data/Harry_Potter_all_books_preprocessed.txt'\n",
    "full_book = './data/hp.txt'\n",
    "\n",
    "text=load_data(full_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aa70777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text.replace(\" .\",\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a6c7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "para=[]\n",
    "\n",
    "for paras in text.split(\"\\n\\n\"):\n",
    "    word_token=word_tokenize(paras)\n",
    "    clean_token=[token for token in word_token if token.casefold() not in stop_words and token.casefold() not in string.punctuation]\n",
    "    para.append(clean_token)\n",
    "\n",
    "save_data('./data/para.json',para)\n",
    "print(len(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f6cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d50c233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "def training(model_name):\n",
    "    with open('./data/para.json','r',encoding='utf-8') as f:\n",
    "        texts=json.load(f)\n",
    "    # print(texts)  \n",
    "    \n",
    "    sentences = texts\n",
    "    \n",
    "    # Getting the number of cores\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Training the word2vec model\n",
    "    w2v_model = Word2Vec(min_count=5, # Minimum word count threshold\n",
    "                     window=2, # The maximum distance between the current and predicted word within a sentence\n",
    "                     vector_size=500, # Embedding vector size\n",
    "                     sample=6e-5, # Downsample setting for frequent words\n",
    "                     alpha=0.03, # Learning rate\n",
    "                     min_alpha=0.0007, # Minimum learning rate\n",
    "                     negative=20, # If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown\n",
    "                     workers=cores-1) # Number of threads to use\n",
    "    \n",
    "    # Building the Vocab\n",
    "    w2v_model.build_vocab(sentences, progress_per=5000)\n",
    "    \n",
    "    # Training the model\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "    \n",
    "    # w2v_model.init_sims(replace=True)\n",
    "    \n",
    "    w2v_model.save(f\"word_vectors/{model_name}.model\")\n",
    "    w2v_model.wv.save_word2vec_format(f\"word_vectors/{model_name}.txt\", binary=False)\n",
    "    print(\"Training Complete\")\n",
    "    \n",
    "    \n",
    "training('hp_ner_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "962be4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_similarity(word):\n",
    "    model = KeyedVectors. load_word2vec_format ('word_vectors/hp_ner_01.txt', binary=False)\n",
    "    results = model.most_similar(positive=[word])\n",
    "    return (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7404ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ron', 0.9997612237930298), ('yelled', 0.9997559785842896), ('noticed', 0.9997556805610657), ('quite', 0.9997545480728149), ('sat', 0.9997537732124329), ('bringing', 0.9997531771659851), ('stood', 0.9997504353523254), ('quickly', 0.9997499585151672), ('sprinted', 0.9997485280036926), ('leaned', 0.9997472763061523)]\n"
     ]
    }
   ],
   "source": [
    "print(gen_similarity('Harry'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c18f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47ebce-8063-4c2b-abff-126b704084b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = KeyedVectors. load_word2vec_format ('word_vectors/word2ve_hp_ner_model.txt', binary=False)\n",
    "\n",
    "\"\"\"\n",
    "SOURCE:\n",
    "https://stackoverflow.com/questions/54717449/mapping-word-vector-to-the-most-similar-closest-word-using-spacy\n",
    "\"\"\"\n",
    "\n",
    "def spacy_similarity(your_word):\n",
    "    nlp = spacy. load (\"en_core_web_1g\")\n",
    "    \n",
    "    ms = nlp. vocab.vectors.most_similar(\n",
    "        np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10)\n",
    "    \n",
    "    words = [nlp.vocab.strings[w] for w in ms [0][0]]\n",
    "    distances = ms[2]\n",
    "    print (words)\n",
    "\n",
    "spacy_similarity(\"albus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41bfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14dfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.scatter(df['x'], df['y'])\n",
    "# for word, pos in df.iterrows ():\n",
    "# plt. annotate(word, pos)\n",
    "# fig. savefig(\"hp_vectors\")\n",
    "# plt.show()\n",
    "\n",
    "# # generate_plot()\n",
    "# #\n",
    "# # def analogy (x1, x2, y1):\n",
    "# #\n",
    "# result = model.most similarpositive=[y1, x2], negative=[x1])\n",
    "# #\n",
    "# #\n",
    "# return result[0:10]\n",
    "# #\n",
    "# # result = analogy('Gryffindor', 'red', 'SLytherin')\n",
    "# # print (result)\n",
    "# #\n",
    "# gen_similarity(\"Gryffindor\")\n",
    "# # display_pca_scatterplot([\"Potter\", \"Ron\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
